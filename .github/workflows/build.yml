name: Build
on:
  pull_request:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: '15 * * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  check-version:
    runs-on: ubuntu-latest
    outputs:
      llama-tag: ${{ steps.tags.outputs.llama-tag }}
      current-tag: ${{ steps.tags.outputs.current-tag }}
    steps:
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v5
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          fetch-tags: 'true'
      - name: Fetch llama tags
        run: |
          cd llama.cpp
          git fetch --prune --prune-tags --unshallow --no-recurse-submodules
      - name: Determine llama.cpp tag name
        id: llamatag
        uses: ./.github/actions/get-tag-name
        with:
          repo-path: ./llama.cpp
      - name: Fetch tags
        run: |
          git fetch --prune --prune-tags --unshallow --no-recurse-submodules
      - name: Determine own tag name
        id: currenttag
        uses: ./.github/actions/get-tag-name
        with:
          repo-path: .
      - name: Save tags
        id: tags
        run: |
          echo "llama-tag=${{ steps.llamatag.outputs.name }}" >> "$GITHUB_OUTPUT"
          echo "current-tag=${{ steps.currenttag.outputs.name }}" >> "$GITHUB_OUTPUT"

  build-cuda-amd64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:12.9.1-devel-ubuntu24.04
      options: --platform linux/amd64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v5
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
            apt update
            apt install -y cmake build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip
      - name: Build amd64
        working-directory: llama.cpp
        run: |
          cmake -S . -B build -G Ninja \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="86;89" \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama.cpp/build/bin/*
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64

  build-cuda-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    container: 
      image: nvidia/cuda:12.9.1-devel-ubuntu22.04
      options: --platform linux/arm64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v5
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
            apt update
            apt install -y cmake build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          cmake -S . -B build -G Ninja \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="87" \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama.cpp/build/bin/*
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64

  build-vulkan-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    steps:
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v5
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          sudo add-apt-repository -y ppa:kisak/kisak-mesa
          sudo apt-get update
          sudo apt-get install -y cmake build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip mesa-vulkan-drivers libxcb-xinput0 libxcb-xinerama0 libxcb-cursor-dev
      - name: Set latest Vulkan SDK version
        id: vulkan_sdk_version
        run: |
          echo "VULKAN_SDK_VERSION=1.4.328.1" >> "$GITHUB_ENV"
      - name: Use Vulkan SDK Cache
        uses: actions/cache@v4
        id: cache-sdk
        with:
          path: ./vulkan_sdk
          key: vulkan-sdk-${{ env.VULKAN_SDK_VERSION }}-${{ runner.os }}
      - name: Setup Vulkan SDK
        if: steps.cache-sdk.outputs.cache-hit != 'true'
        uses: ./.github/actions/linux-setup-vulkan
        with:
          path: ./vulkan_sdk
          version: ${{ env.VULKAN_SDK_VERSION }}
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          source $GITHUB_WORKSPACE/vulkan_sdk/setup-env.sh
          cmake -S . -B build -G Ninja \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_VULKAN=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama.cpp/build/bin/*
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64

  release:
    needs: 
      - check-version
      - build-cuda-amd64
      - build-cuda-arm64
      - build-vulkan-arm64
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Download amd64 CUDA artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Download arm64 CUDA artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Download arm64 Vulkan artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Create final release
        uses: ncipollo/release-action@v1
        with:
          tag: ${{ needs.check-version.outputs.llama-tag }}
          commit: main
          body: |
            ## Changes in llama.cpp ${{ needs.check-version.outputs.llama-tag }}
            See the [llama.cpp releases](https://github.com/ggml-org/llama.cpp/releases/tag/${{ needs.check-version.outputs.llama-tag }}) for details.
          artifacts: "*.zip"

  trigger-yzma-tests:
    needs: release
    runs-on: ubuntu-latest
    steps:
      - name: Trigger yzma repo Linux build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/linux.yml/dispatches \
          -d '{"ref": "main"}'
      - name: Trigger yzma repo macOS build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/macos.yml/dispatches \
          -d '{"ref": "main"}'
      - name: Trigger yzma repo Windows build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/windows.yml/dispatches \
          -d '{"ref": "main"}'
