name: Build
on:
  pull_request:
  push:
    branches:
      - main
  workflow_dispatch:
  # schedule:
  #   - cron: '15 * * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  check-version:
    runs-on: ubuntu-latest
    outputs:
      llama-tag: ${{ steps.tags.outputs.llama-tag }}
      current-tag: ${{ steps.tags.outputs.current-tag }}
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          fetch-tags: 'true'
      - name: Fetch llama tags
        run: |
          cd llama.cpp
          git fetch --prune --prune-tags --unshallow --no-recurse-submodules
      - name: Determine llama.cpp tag name
        id: llamatag
        uses: ./.github/actions/get-tag-name
        with:
          repo-path: ./llama.cpp
      - name: Fetch tags
        run: |
          git fetch --prune --prune-tags --unshallow --no-recurse-submodules
      - name: Determine own tag name
        id: currenttag
        uses: ./.github/actions/get-tag-name
        with:
          repo-path: .
      - name: Save tags
        id: tags
        run: |
          echo "llama-tag=${{ steps.llamatag.outputs.name }}" >> "$GITHUB_OUTPUT"
          echo "current-tag=${{ steps.currenttag.outputs.name }}" >> "$GITHUB_OUTPUT"

  build-cuda-amd64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:12.9.1-devel-ubuntu24.04
      options: --platform linux/amd64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
            apt update
            apt install -y cmake build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip
      - name: Build amd64
        working-directory: llama.cpp
        run: |
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="86;89" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_CUDA_CUB_3DOT2=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64

  build-cuda-13-amd64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:13.0.2-devel-ubuntu24.04
      options: --platform linux/amd64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
            apt update
            apt install -y cmake build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip
      - name: Build amd64
        working-directory: llama.cpp
        run: |
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="86;89" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_CUDA_CUB_3DOT2=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64

  build-cuda-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    container: 
      image: nvidia/cuda:12.9.1-devel-ubuntu22.04
      options: --platform linux/arm64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          apt update
          apt install -y build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip libmpfr-dev libgmp-dev libmpc-dev wget
      - name: Cache GCC 14.3.0
        uses: actions/cache@v5
        id: cache-gcc
        with:
          path: /usr/local/gcc-14.3.0
          key: gcc-14.3.0-arm64-cuda-ubuntu22.04-v2
      - name: Build GCC 14.3.0
        if: steps.cache-gcc.outputs.cache-hit != 'true'
        run: |
          wget http://ftp.gnu.org/gnu/gcc/gcc-14.3.0/gcc-14.3.0.tar.gz
          tar -xf gcc-14.3.0.tar.gz
          cd gcc-14.3.0
          ./configure -v --build=$(uname -m)-linux-gnu --host=$(uname -m)-linux-gnu --target=$(uname -m)-linux-gnu --prefix=/usr/local/gcc-14.3.0 --enable-checking=release --enable-languages=c,c++ --disable-multilib --program-suffix=-14
          make -j $(nproc)
          make install
          cd ..
          rm -rf gcc-14.3.0 gcc-14.3.0.tar.gz
      - name: Add GCC to PATH
        run: |
          echo "/usr/local/gcc-14.3.0/bin" >> $GITHUB_PATH
      - name: Setup cmake 4.2.0
        run: |
          mkdir -p ./cmake-4.2.0
          cd ./cmake-4.2.0
          wget -O cmake.sh https://github.com/Kitware/CMake/releases/download/v4.2.0/cmake-4.2.0-linux-aarch64.sh
          sh cmake.sh --prefix=/usr/local --skip-license
          cd ..
          rm -rf cmake-4.2.0
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          export CC=/usr/local/gcc-14.3.0/bin/gcc-14
          export CXX=/usr/local/gcc-14.3.0/bin/g++-14
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="87" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_CUDA_CUB_3DOT2=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64

  build-cuda-13-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    container:
      image: nvidia/cuda:13.0.2-devel-ubuntu22.04
      options: --platform linux/arm64
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          apt update
          apt install -y build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip libmpfr-dev libgmp-dev libmpc-dev wget
      - name: Cache GCC 14.3.0
        uses: actions/cache@v5
        id: cache-gcc
        with:
          path: /usr/local/gcc-14.3.0
          key: gcc-14.3.0-arm64-cuda-13-ubuntu22.04-v2
      - name: Build GCC 14.3.0
        if: steps.cache-gcc.outputs.cache-hit != 'true'
        run: |
          wget http://ftp.gnu.org/gnu/gcc/gcc-14.3.0/gcc-14.3.0.tar.gz
          tar -xf gcc-14.3.0.tar.gz
          cd gcc-14.3.0
          ./configure -v --build=$(uname -m)-linux-gnu --host=$(uname -m)-linux-gnu --target=$(uname -m)-linux-gnu --prefix=/usr/local/gcc-14.3.0 --enable-checking=release --enable-languages=c,c++ --disable-multilib --program-suffix=-14
          make -j $(nproc)
          make install
          cd ..
          rm -rf gcc-14.3.0 gcc-14.3.0.tar.gz
      - name: Add GCC to PATH
        run: |
          echo "/usr/local/gcc-14.3.0/bin" >> $GITHUB_PATH
      - name: Setup cmake 4.2.0
        run: |
          mkdir -p ./cmake-4.2.0
          cd ./cmake-4.2.0
          wget -O cmake.sh https://github.com/Kitware/CMake/releases/download/v4.2.0/cmake-4.2.0-linux-aarch64.sh
          sh cmake.sh --prefix=/usr/local --skip-license
          cd ..
          rm -rf cmake-4.2.0
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          export CC=/usr/local/gcc-14.3.0/bin/gcc-14
          export CXX=/usr/local/gcc-14.3.0/bin/g++-14
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DCMAKE_CUDA_ARCHITECTURES="87" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_CUDA=ON \
            -DGGML_CUDA_CUB_3DOT2=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64

  build-vulkan-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          sudo add-apt-repository -y ppa:kisak/kisak-mesa
          sudo apt-get update
          sudo apt-get install -y build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip mesa-vulkan-drivers libxcb-xinput0 libxcb-xinerama0 libxcb-cursor-dev wget libgmp-dev libmpfr-dev libmpc-dev
      - name: Cache GCC 14.3.0
        uses: actions/cache@v5
        id: cache-gcc-vulkan
        with:
          path: ./gcc-14.3.0
          key: gcc-14.3.0-arm64-vulkan-ubuntu22.04-v3
      - name: Build GCC 14.3.0
        if: steps.cache-gcc-vulkan.outputs.cache-hit != 'true'
        run: |
          mkdir -p gcc-install
          cd gcc-install
          wget http://ftp.gnu.org/gnu/gcc/gcc-14.3.0/gcc-14.3.0.tar.gz
          tar -xf gcc-14.3.0.tar.gz
          cd gcc-14.3.0
          ./configure -v --build=$(uname -m)-linux-gnu --host=$(uname -m)-linux-gnu --target=$(uname -m)-linux-gnu --prefix=$GITHUB_WORKSPACE/gcc-14.3.0 --enable-checking=release --enable-languages=c,c++ --disable-multilib --program-suffix=-14
          make -j $(nproc)
          sudo make install
          cd ../..
          rm -rf gcc-install
      - name: Add GCC to PATH
        run: |
          echo "$GITHUB_WORKSPACE/gcc-14.3.0/bin" >> $GITHUB_PATH
      - name: Setup cmake 4.2.0
        run: |
          mkdir -p ./cmake-4.2.0
          cd ./cmake-4.2.0
          wget -O cmake.sh https://github.com/Kitware/CMake/releases/download/v4.2.0/cmake-4.2.0-linux-aarch64.sh
          sudo sh cmake.sh --prefix=/usr/local --skip-license
          cd ..
          rm -rf cmake-4.2.0
      - name: Set latest Vulkan SDK version
        id: vulkan_sdk_version
        run: |
          echo "VULKAN_SDK_VERSION=1.4.335.0" >> "$GITHUB_ENV"
      - name: Use Vulkan SDK Cache
        uses: actions/cache@v5
        id: cache-sdk
        with:
          path: ./vulkan_sdk
          key: vulkan-sdk-${{ env.VULKAN_SDK_VERSION }}-${{ runner.os }}
      - name: Setup Vulkan SDK
        if: steps.cache-sdk.outputs.cache-hit != 'true'
        uses: ./.github/actions/linux-setup-vulkan
        with:
          path: ./vulkan_sdk
          version: ${{ env.VULKAN_SDK_VERSION }}
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          export CC=$GITHUB_WORKSPACE/gcc-14.3.0/bin/gcc-14
          export CXX=$GITHUB_WORKSPACE/gcc-14.3.0/bin/g++-14
          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/gcc-14.3.0/lib64:$LD_LIBRARY_PATH
          source $GITHUB_WORKSPACE/vulkan_sdk/setup-env.sh
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_VULKAN=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64

  build-cpu-arm64:
    needs: check-version
    if: ${{ needs.check-version.outputs.llama-tag != needs.check-version.outputs.current-tag }}
    runs-on: ubuntu-22.04-arm
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v6
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          ref: ${{ needs.check-version.outputs.llama-tag }}
          fetch-tags: 'true'
      - name: Install dependencies
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential ninja-build libgomp1 git libssl-dev libcurl4-openssl-dev zip libxcb-xinput0 libxcb-xinerama0 libxcb-cursor-dev wget libgmp-dev libmpfr-dev libmpc-dev
      - name: Cache GCC 14.3.0
        uses: actions/cache@v5
        id: cache-gcc-cpu
        with:
          path: ./gcc-14.3.0
          key: gcc-14.3.0-arm64-cpu-ubuntu22.04-v1
      - name: Build GCC 14.3.0
        if: steps.cache-gcc-cpu.outputs.cache-hit != 'true'
        run: |
          mkdir -p gcc-install
          cd gcc-install
          wget http://ftp.gnu.org/gnu/gcc/gcc-14.3.0/gcc-14.3.0.tar.gz
          tar -xf gcc-14.3.0.tar.gz
          cd gcc-14.3.0
          ./configure -v --build=$(uname -m)-linux-gnu --host=$(uname -m)-linux-gnu --target=$(uname -m)-linux-gnu --prefix=$GITHUB_WORKSPACE/gcc-14.3.0 --enable-checking=release --enable-languages=c,c++ --disable-multilib --program-suffix=-14
          make -j $(nproc)
          sudo make install
          cd ../..
          rm -rf gcc-install
      - name: Add GCC to PATH
        run: |
          echo "$GITHUB_WORKSPACE/gcc-14.3.0/bin" >> $GITHUB_PATH
      - name: Setup cmake 4.2.0
        run: |
          mkdir -p ./cmake-4.2.0
          cd ./cmake-4.2.0
          wget -O cmake.sh https://github.com/Kitware/CMake/releases/download/v4.2.0/cmake-4.2.0-linux-aarch64.sh
          sudo sh cmake.sh --prefix=/usr/local --skip-license
          cd ..
          rm -rf cmake-4.2.0
      - name: Build arm64
        working-directory: llama.cpp
        run: |
          export CC=$GITHUB_WORKSPACE/gcc-14.3.0/bin/gcc-14
          export CXX=$GITHUB_WORKSPACE/gcc-14.3.0/bin/g++-14
          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/gcc-14.3.0/lib64:$LD_LIBRARY_PATH
          cmake -S . -B build -G Ninja \
            -DCMAKE_INSTALL_RPATH='$ORIGIN' \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined" \
            -DGGML_NATIVE=OFF \
            -DGGML_CPU_ALL_VARIANTS=ON \
            -DGGML_BACKEND_DL=ON
          cmake --build build --config Release -j $(nproc)
      - name: Pack artifacts
        run: |
          tar -czvf llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64.tar.gz --transform "s,./,llama-${{ needs.check-version.outputs.llama-tag }}/," -C ./llama.cpp/build/bin .
      - name: Upload release binaries
        uses: actions/upload-artifact@v5
        with:
          path: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64.tar.gz
          name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64

  release:
    needs: 
      - check-version
      - build-cuda-amd64
      - build-cuda-13-amd64
      - build-cuda-arm64
      - build-cuda-13-arm64
      - build-vulkan-arm64
      - build-cpu-arm64
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6
      - name: Download amd64 CUDA artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract amd64 CUDA artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-x64/*.tar.gz .
      - name: Download amd64 CUDA 13 artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract amd64 CUDA artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-x64/*.tar.gz .
      - name: Download arm64 CUDA artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract arm64 CUDA artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-arm64/*.tar.gz .
      - name: Download arm64 CUDA 13 artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract arm64 CUDA artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cuda-13-arm64/*.tar.gz .
      - name: Download arm64 Vulkan artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract arm64 Vulkan artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-vulkan-arm64/*.tar.gz .
      - name: Download arm64 CPU artifact
        uses: ./.github/actions/download-artifact
        with:
          run-id: ${{ github.run_id }}
          artifact-name: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64
          destination: llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract arm64 CPU artifact
        run: |
          unzip llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64.zip -d llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64
          mv llama-${{ needs.check-version.outputs.llama-tag }}-bin-ubuntu-cpu-arm64/*.tar.gz .
      - name: Create final release
        uses: ncipollo/release-action@v1
        with:
          tag: ${{ needs.check-version.outputs.llama-tag }}
          commit: main
          body: |
            ## Changes in llama.cpp ${{ needs.check-version.outputs.llama-tag }}
            See the [llama.cpp releases](https://github.com/ggml-org/llama.cpp/releases/tag/${{ needs.check-version.outputs.llama-tag }}) for details.
          artifacts: "*.tar.gz"

  trigger-yzma-tests:
    needs: release
    runs-on: ubuntu-latest
    steps:
      - name: Trigger yzma repo Linux build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/linux.yml/dispatches \
          -d '{"ref": "main"}'
      - name: Trigger yzma repo macOS build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/macos.yml/dispatches \
          -d '{"ref": "main"}'
      - name: Trigger yzma repo Windows build
        run: |
          curl -X POST \
          -H "Authorization: Bearer ${{secrets.YZMA_BUILD_ACCESS}}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/hybridgroup/yzma/actions/workflows/windows.yml/dispatches \
          -d '{"ref": "main"}'
