name: Build
on:
  pull_request:
  push:
    branches:
      - main
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-cuda:
    runs-on: ubuntu-latest
    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Check out the repo
        uses: actions/checkout@v5
      - name: Checkout llama.cpp repo
        uses: actions/checkout@v5
        with:
          repository: ggml-org/llama.cpp
          path: llama.cpp
          fetch-tags: 'true'
      - name: Determine llama.cpp tag name
        id: tag
        uses: ./.github/actions/get-tag-name
        with:
          repo-path: ./llama.cpp
      - name: Build amd64
        run: |
          docker pull --platform linux/amd64 ghcr.io/hybridgroup/llama-cpp-builder-cuda-12.9:latest
          docker run --rm -v $(pwd):/src -a stdout -a stderr --platform linux/amd64 ghcr.io/hybridgroup/llama-cpp-builder-cuda-12.9:latest
      - name: Compress release binaries
        run: |
          zip llama-${{ steps.tag.outputs.name }}-bin-ubuntu-cuda-x64.zip llama.cpp/build/bin/*
      - name: Create release
        uses: ncipollo/release-action@v1
        with:
          artifacts: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-cuda-x64.zip
          tag: ${{ steps.tag.outputs.name }}
          commit: main
